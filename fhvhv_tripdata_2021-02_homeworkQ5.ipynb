{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d04b59",
   "metadata": {},
   "source": [
    "# Try this with Dask First"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94c06f",
   "metadata": {},
   "source": [
    "As with Question 5 I'm going to try this firstly with Dask and then with Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350fdb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03_test.ipynb\t       fhvhv_tripdata_2021-01.csv\r\n",
      "04_pyspark.ipynb       fhvhv_tripdata_2021-01.csv.1\r\n",
      "05_taxi_schema.ipynb   fhvhv_tripdata_2021-02.csv\r\n",
      "06_spark_sql.ipynb     fhvhv_tripdata_2021-02_homeworkQ3.ipynb\r\n",
      "07_groupby_join.ipynb  fhvhv_tripdata_2021-02_homeworkQ4.ipynb\r\n",
      "08_rdds.ipynb\t       head.csv\r\n",
      "data\t\t       HomeworkQ3.ipynb\r\n",
      "download_data.sh       test.py\r\n",
      "fhvhv\r\n"
     ]
    }
   ],
   "source": [
    "#just finding my exact file name here...\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdfd6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a613d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'fhvhv_tripdata_2021-02.csv'\n",
    "df = dd.read_csv(filename, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95cc151b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "value_counts_df = df['dispatching_base_num'].value_counts().compute()\n",
    "#print(df.compute()) # this step took around 5 minutes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411e578",
   "metadata": {},
   "source": [
    "# Answer in Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc20b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B02510    3233664\n",
       "B02764     965568\n",
       "B02872     882689\n",
       "B02875     685390\n",
       "B02765     559768\n",
       "B02869     429720\n",
       "B02887     322331\n",
       "B02871     312364\n",
       "B02864     311603\n",
       "B02866     311089\n",
       "B02878     305185\n",
       "B02682     303255\n",
       "B02617     274510\n",
       "B02883     251617\n",
       "B02884     244963\n",
       "B02882     232173\n",
       "B02876     215693\n",
       "B02879     210137\n",
       "B02867     200530\n",
       "B02877     198938\n",
       "B02835     189031\n",
       "B02888     169167\n",
       "B02889     138762\n",
       "B02836     128978\n",
       "B02880     115716\n",
       "B02395     112433\n",
       "B02870     101945\n",
       "B02800      84277\n",
       "B02865      76160\n",
       "B02512      41043\n",
       "B02844       3502\n",
       "B03136       1741\n",
       "Name: dispatching_base_num, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4d443",
   "metadata": {},
   "source": [
    "# Now try with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07de9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca5bbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('fhvhv_tripdata_2021-02_homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf8de204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a52087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wc -l fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16937bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc61a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec05f6",
   "metadata": {},
   "source": [
    "I don't need to bother repartitioning etc. here as I've already done it before. I'll turn these code cells into markdown cells and simply start off from reading the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb5783",
   "metadata": {},
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f3c1a",
   "metadata": {},
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e3bfc",
   "metadata": {},
   "source": [
    "df.write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3cab876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "203b5627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e809c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:=====================================>                (139 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  count|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "|              B02764| 965568|\n",
      "|              B02872| 882689|\n",
      "|              B02875| 685390|\n",
      "|              B02765| 559768|\n",
      "|              B02869| 429720|\n",
      "|              B02887| 322331|\n",
      "|              B02871| 312364|\n",
      "|              B02864| 311603|\n",
      "|              B02866| 311089|\n",
      "|              B02878| 305185|\n",
      "|              B02682| 303255|\n",
      "|              B02617| 274510|\n",
      "|              B02883| 251617|\n",
      "|              B02884| 244963|\n",
      "|              B02882| 232173|\n",
      "|              B02876| 215693|\n",
      "|              B02879| 210137|\n",
      "|              B02867| 200530|\n",
      "|              B02877| 198938|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:=============================================>        (170 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# One way...\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_grouped_desc = df \\\n",
    "    .groupBy(['dispatching_base_num']).count() \\\n",
    "    .sort(desc('count')) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0e47c",
   "metadata": {},
   "source": [
    "# Answer in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7d4e72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('fhvhv_tripdata_2021-02_homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7919d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "acedb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84ea4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:=======================================>              (146 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|dispatching_base_num|count(1)|\n",
      "+--------------------+--------+\n",
      "|              B02510| 3233664|\n",
      "|              B02764|  965568|\n",
      "|              B02872|  882689|\n",
      "|              B02875|  685390|\n",
      "|              B02765|  559768|\n",
      "|              B02869|  429720|\n",
      "|              B02887|  322331|\n",
      "|              B02871|  312364|\n",
      "|              B02864|  311603|\n",
      "|              B02866|  311089|\n",
      "|              B02878|  305185|\n",
      "|              B02682|  303255|\n",
      "|              B02617|  274510|\n",
      "|              B02883|  251617|\n",
      "|              B02884|  244963|\n",
      "|              B02882|  232173|\n",
      "|              B02876|  215693|\n",
      "|              B02879|  210137|\n",
      "|              B02867|  200530|\n",
      "|              B02877|  198938|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 70:===================================================>  (192 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"fhvhv_2021_02\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT dispatching_base_num, \\\n",
    "                          count(1) \\\n",
    "                     FROM fhvhv_2021_02 \\\n",
    "                    GROUP BY dispatching_base_num \\\n",
    "                    ORDER BY count(1) DESC\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf9359b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(trip_length_sec)|\n",
      "+--------------------+\n",
      "|               75540|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Getting just the max time\n",
    "sqlDF = spark.sql(\"SELECT MAX(trip_length_sec) \\\n",
    "                     FROM fhvhv_2021_02_trip_duration\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a8fefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|trip_length_sec|\n",
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "|           HV0005|              B02510|2021-02-11 13:40:44|2021-02-12 10:39:44|          75540|\n",
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:===================>                                       (1 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Combine the above into another way of finding out\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT hvfhs_license_num, dispatching_base_num, \\\n",
    "                          pickup_datetime, dropoff_datetime, trip_length_sec \\\n",
    "                     FROM fhvhv_2021_02_trip_duration \\\n",
    "                    WHERE trip_length_sec == ( \\\n",
    "                        SELECT MAX(trip_length_sec) \\\n",
    "                          FROM fhvhv_2021_02_trip_duration) \\\n",
    "                  \")\n",
    "sqlDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
