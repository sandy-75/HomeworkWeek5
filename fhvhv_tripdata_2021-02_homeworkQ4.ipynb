{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d04b59",
   "metadata": {},
   "source": [
    "# Try this with Dask First"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94c06f",
   "metadata": {},
   "source": [
    "As with Question 3 I'm going to try this firstly with Dask and then with Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350fdb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03_test.ipynb\t       fhvhv_tripdata_2021-01.csv\r\n",
      "04_pyspark.ipynb       fhvhv_tripdata_2021-01.csv.1\r\n",
      "05_taxi_schema.ipynb   fhvhv_tripdata_2021-02.csv\r\n",
      "06_spark_sql.ipynb     fhvhv_tripdata_2021-02_homeworkQ3.ipynb\r\n",
      "07_groupby_join.ipynb  fhvhv_tripdata_2021-02_homeworkQ4.ipynb\r\n",
      "08_rdds.ipynb\t       head.csv\r\n",
      "data\t\t       HomeworkQ3.ipynb\r\n",
      "download_data.sh       test.py\r\n",
      "fhvhv\r\n"
     ]
    }
   ],
   "source": [
    "#just finding my exact file name here...\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdfd6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a613d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'fhvhv_tripdata_2021-02.csv'\n",
    "df = dd.read_csv(filename, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95cc151b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['trip_duration'] = df['dropoff_datetime'] - df['pickup_datetime']\n",
    "#print(df.compute()) # this step took around 5 minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5321d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num                object\n",
       "dispatching_base_num             object\n",
       "pickup_datetime          datetime64[ns]\n",
       "dropoff_datetime         datetime64[ns]\n",
       "PULocationID                      int64\n",
       "DOLocationID                      int64\n",
       "SR_Flag                         float64\n",
       "trip_duration           timedelta64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aa69a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trip_time = df['trip_duration'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00caa71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 20:59:00')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_trip_time.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411e578",
   "metadata": {},
   "source": [
    "# Answer in Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cafd92d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139863</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>2021-02-11 13:40:44</td>\n",
       "      <td>2021-02-12 10:39:44</td>\n",
       "      <td>247</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 20:59:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hvfhs_license_num dispatching_base_num     pickup_datetime  \\\n",
       "139863            HV0005               B02510 2021-02-11 13:40:44   \n",
       "\n",
       "          dropoff_datetime  PULocationID  DOLocationID  SR_Flag  \\\n",
       "139863 2021-02-12 10:39:44           247            41      NaN   \n",
       "\n",
       "         trip_duration  \n",
       "139863 0 days 20:59:00  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_trips = df[(df.trip_duration == max_trip_time)]\n",
    "longest_trips.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4d443",
   "metadata": {},
   "source": [
    "# Now try with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07de9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5bbb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 10:15:15 WARN Utils: Your hostname, sandy-ODYSSEY-X86J4105 resolves to a loopback address: 127.0.1.1; using 10.0.0.1 instead (on interface wlo2)\n",
      "22/03/01 10:15:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/sandy/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/01 10:15:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('fhvhv_tripdata_2021-02_homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8de204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a52087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wc -l fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16937bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc61a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec05f6",
   "metadata": {},
   "source": [
    "I don't need to bother repartitioning etc. here as I've already done it before. I'll turn these code cells into markdown cells and simply start off from reading the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb5783",
   "metadata": {},
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f3c1a",
   "metadata": {},
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e3bfc",
   "metadata": {},
   "source": [
    "df.write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3cab876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203b5627",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ae734ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column - see https://sparkbyexamples.com/pyspark/pyspark-timestamp-difference-seconds-minutes-hours/\n",
    "# get the difference in seconds using unix_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_trip_length = df \\\n",
    "    .withColumn('trip_length_sec',\n",
    "                 unix_timestamp('dropoff_datetime') - unix_timestamp('pickup_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847a95ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- trip_length_sec: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_length.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0e47c",
   "metadata": {},
   "source": [
    "# Answer in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84ea4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|trip_length_sec|\n",
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "|           HV0005|              B02510|2021-02-11 13:40:44|2021-02-12 10:39:44|          75540|\n",
      "|           HV0004|              B02800|2021-02-17 15:54:53|2021-02-18 07:48:34|          57221|\n",
      "|           HV0004|              B02800|2021-02-20 12:08:15|2021-02-21 00:22:14|          44039|\n",
      "|           HV0003|              B02864|2021-02-03 20:24:25|2021-02-04 07:41:58|          40653|\n",
      "|           HV0003|              B02887|2021-02-19 23:17:44|2021-02-20 09:44:01|          37577|\n",
      "|           HV0003|              B02764|2021-02-25 17:13:35|2021-02-26 02:57:05|          35010|\n",
      "|           HV0003|              B02875|2021-02-20 01:36:13|2021-02-20 11:16:19|          34806|\n",
      "|           HV0005|              B02510|2021-02-18 15:24:19|2021-02-19 01:01:11|          34612|\n",
      "|           HV0003|              B02764|2021-02-18 01:31:20|2021-02-18 11:07:15|          34555|\n",
      "|           HV0005|              B02510|2021-02-10 20:51:39|2021-02-11 06:21:08|          34169|\n",
      "|           HV0003|              B02764|2021-02-10 01:56:17|2021-02-10 10:57:33|          32476|\n",
      "|           HV0005|              B02510|2021-02-25 09:18:18|2021-02-25 18:18:57|          32439|\n",
      "|           HV0005|              B02510|2021-02-21 19:59:13|2021-02-22 04:56:16|          32223|\n",
      "|           HV0003|              B02864|2021-02-09 18:36:13|2021-02-10 03:31:00|          32087|\n",
      "|           HV0004|              B02800|2021-02-06 09:48:09|2021-02-06 18:32:16|          31447|\n",
      "|           HV0005|              B02510|2021-02-02 09:42:30|2021-02-02 18:17:43|          30913|\n",
      "|           HV0005|              B02510|2021-02-10 10:12:08|2021-02-10 18:46:24|          30856|\n",
      "|           HV0003|              B02764|2021-02-09 13:30:13|2021-02-09 22:02:25|          30732|\n",
      "|           HV0005|              B02510|2021-02-21 22:50:52|2021-02-22 07:21:52|          30660|\n",
      "|           HV0005|              B02510|2021-02-05 21:32:33|2021-02-06 06:01:04|          30511|\n",
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df_trip_length.createOrReplaceTempView(\"fhvhv_2021_02_trip_duration\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT hvfhs_license_num, dispatching_base_num, \\\n",
    "                          pickup_datetime, dropoff_datetime, trip_length_sec \\\n",
    "                     FROM fhvhv_2021_02_trip_duration \\\n",
    "                    ORDER BY trip_length_sec DESC\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf9359b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(trip_length_sec)|\n",
      "+--------------------+\n",
      "|               75540|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Getting just the max time\n",
    "sqlDF = spark.sql(\"SELECT MAX(trip_length_sec) \\\n",
    "                     FROM fhvhv_2021_02_trip_duration\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a8fefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|trip_length_sec|\n",
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "|           HV0005|              B02510|2021-02-11 13:40:44|2021-02-12 10:39:44|          75540|\n",
      "+-----------------+--------------------+-------------------+-------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:===================>                                       (1 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Combine the above into another way of finding out\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT hvfhs_license_num, dispatching_base_num, \\\n",
    "                          pickup_datetime, dropoff_datetime, trip_length_sec \\\n",
    "                     FROM fhvhv_2021_02_trip_duration \\\n",
    "                    WHERE trip_length_sec == ( \\\n",
    "                        SELECT MAX(trip_length_sec) \\\n",
    "                          FROM fhvhv_2021_02_trip_duration) \\\n",
    "                  \")\n",
    "sqlDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
