{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a4d443",
   "metadata": {},
   "source": [
    "# Pyspark join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07de9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import concat, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca5bbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('fhvhv_tripdata_2021-02_homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf8de204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a52087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wc -l fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc61a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema = types.StructType([\n",
    "#    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "#    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "#    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "#    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "#    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "#    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "#    types.StructField('SR_Flag', types.StringType(), True)\n",
    "#])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec05f6",
   "metadata": {},
   "source": [
    "I don't need to bother repartitioning etc. here as I've already done it before. I'll turn these code cells into markdown cells and simply start off from reading the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb5783",
   "metadata": {},
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f3c1a",
   "metadata": {},
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e3bfc",
   "metadata": {},
   "source": [
    "df.write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3cab876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark.read.parquet('fhvhv/2021/02/')\n",
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5718cd9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check how many rows in the df_fhvhv dataframe...\n",
    "df_fhvhv.createOrReplaceTempView(\"fhvhv_2021_02\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT COUNT(1) \\\n",
    "                     FROM fhvhv_2021_02\")\n",
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "073d06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just check how many rows in the df_zones dataframe...\n",
    "df_zones.createOrReplaceTempView(\"taxi_zones\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT COUNT(1) \\\n",
    "                     FROM taxi_zones\")\n",
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1962f",
   "metadata": {},
   "source": [
    "There are many different types of joins in PySpark that support traditional SQL joins.\n",
    "However, here we want to preserve everything that is in our fhvhv dataset when we join to the zone data.\n",
    "If we do this we must do an outer join. Note that the default join in pyspark is an inner\n",
    "join and will this ould have the result of dropping any rows that don't have a key in common (the one that we are joining on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5704a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_1 = df_fhvhv.join(df_zones, df_fhvhv.PULocationID == df_zones.LocationID, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff242522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check again how many rows in the joined dataframe. It should be the same as \n",
    "# fhvhv dataset before we joined. \n",
    "df_join_1.createOrReplaceTempView(\"df_join_PU\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT COUNT(1) \\\n",
    "                     FROM df_join_PU\")\n",
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45beee",
   "metadata": {},
   "source": [
    "Note that in the above this isn't exactly what I was expecting - there are more rows here than in the original fhvhv dataframe than when I started.\n",
    "This is because I've done a full outer join. That means if there are any rows in the zone\n",
    "dataframe that don't have a match it will be put here as well. This is not what we are after. Let's change this to a Left Outer Join. This will tell the join to just make sure to grab everything on the left dataset (but not the right) regardless of whether there is a match on the right data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "069feeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the join again but with a left outer join this time\n",
    "df_join_1 = df_fhvhv.join(df_zones, \\\n",
    "                          df_fhvhv.PULocationID == df_zones.LocationID, \\\n",
    "                          how='leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01594f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check again how many rows in the joined dataframe. It should be the same as \n",
    "# fhvhv dataset before we joined. \n",
    "df_join_1.createOrReplaceTempView(\"df_join_PU\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT COUNT(1) \\\n",
    "                     FROM df_join_PU\")\n",
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad2ddd",
   "metadata": {},
   "source": [
    "Okay, that's better. Now let's rename the zone column so we don't get confused down the track when we are doing more joining..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06049296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do some renaming here to save ourselves a bit of confusion\n",
    "df_join_1 = df_join_1 \\\n",
    "    .withColumnRenamed('Zone', 'PU_Zone') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "481595e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9482c71",
   "metadata": {},
   "source": [
    "Now we need to find out the zone names for the drop off location. We will do the same thing as we did for the pickup location. Remembering of course that this is a left outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eca14d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_2 = df_fhvhv.join(df_zones, \\\n",
    "                          df_fhvhv.DOLocationID == df_zones.LocationID, \\\n",
    "                          how = 'leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "976c637c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check again how many rows are in the joined dataframe. It should again be the same as\n",
    "# fhvhv dataset before we joined. \n",
    "df_join_2.createOrReplaceTempView(\"df_join_DO\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT COUNT(1) \\\n",
    "                     FROM df_join_DO\")\n",
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4ec77",
   "metadata": {},
   "source": [
    "That looks good. But let's do some renaming again so we don't get confused when we join the two tables together (df_join_1 + df_join_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4791b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will again rename our zone column \n",
    "df_join_2 = df_join_2 \\\n",
    "    .withColumnRenamed('Zone', 'DO_Zone') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "889c2e3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a758b5",
   "metadata": {},
   "source": [
    "We now do what is essentially a self-join. It would have been better if we had a unique id for each row but we don't. Hence things get a bit ugly. I'm going to say that as long as the hvfhs_license_num, dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID and DOLocationID are the same in each then it is ok to join - this is what uniquely identifies a row. This is terribly inefficient and setting up a UID would have been much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e64d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_1_and_2 = df_join_1.join(df_join_2, (\n",
    "    df_join_1.hvfhs_license_num == df_join_2.hvfhs_license_num) \\\n",
    "    & (df_join_1.dispatching_base_num == df_join_2.dispatching_base_num) \\\n",
    "    & (df_join_1.pickup_datetime == df_join_2.pickup_datetime) \\\n",
    "    & (df_join_1.dropoff_datetime == df_join_2.dropoff_datetime) \\\n",
    "    & (df_join_1.PULocationID == df_join_2.PULocationID) \\\n",
    "    & (df_join_1.DOLocationID == df_join_2.DOLocationID) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a674b4d",
   "metadata": {},
   "source": [
    "Again, let's check the count on this dataframe - it should still be the same as what we\n",
    "originally started with in the df_fhvhv dataframe. If it's not something has gone wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ec3d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a count check\n",
    "df_join_1_and_2.createOrReplaceTempView(\"df_join_PU_DO\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT COUNT(1) \\\n",
    "                     FROM df_join_PU_DO\")\n",
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58219caa",
   "metadata": {},
   "source": [
    "Now it's time to get the column we were after all along. The combination of PU_Zone/DO_Zone so that we can then see which combination is the most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f556c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_3 = df_join_1_and_2 \\\n",
    "    .withColumn('PU_DO_Zone', concat(col('PU_Zone'), lit('/'), col('DO_Zone')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f59781ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's have a look at the joined dataframe and see if we have our new column here\n",
    "# sqlDF.show() # Comment this out for Bonus Question stages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4715a3",
   "metadata": {},
   "source": [
    "Now comes the moment of truth. Which are the most popular PU/DO Zone combinations? Let's do a Group By on the PU/DO Zone combo and then an Order By on the count in descending order to get the most popular first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c838c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:====================================================> (193 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+--------+\n",
      "|PU_DO_Zone                                         |count(1)|\n",
      "+---------------------------------------------------+--------+\n",
      "|East New York/East New York                        |45041   |\n",
      "|Borough Park/Borough Park                          |37329   |\n",
      "|Canarsie/Canarsie                                  |28026   |\n",
      "|Crown Heights North/Crown Heights North            |25976   |\n",
      "|Bay Ridge/Bay Ridge                                |17934   |\n",
      "|Jackson Heights/Jackson Heights                    |14688   |\n",
      "|Astoria/Astoria                                    |14688   |\n",
      "|Central Harlem North/Central Harlem North          |14481   |\n",
      "|Bushwick South/Bushwick South                      |14424   |\n",
      "|Flatbush/Ditmas Park/Flatbush/Ditmas Park          |13976   |\n",
      "|South Ozone Park/South Ozone Park                  |13716   |\n",
      "|Brownsville/Brownsville                            |12829   |\n",
      "|JFK Airport/NA                                     |12542   |\n",
      "|Prospect-Lefferts Gardens/Crown Heights North      |11814   |\n",
      "|Forest Hills/Forest Hills                          |11548   |\n",
      "|Bushwick North/Bushwick South                      |11491   |\n",
      "|Bushwick South/Bushwick North                      |11487   |\n",
      "|Crown Heights North/Prospect-Lefferts Gardens      |11462   |\n",
      "|Crown Heights North/Stuyvesant Heights             |11342   |\n",
      "|Prospect-Lefferts Gardens/Prospect-Lefferts Gardens|11308   |\n",
      "+---------------------------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 83:=====================================================>(197 + 3) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join_3.createOrReplaceTempView(\"pu_do_zones\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT PU_DO_Zone, \\\n",
    "                          count(1) \\\n",
    "                     FROM pu_do_zones \\\n",
    "                    GROUP BY PU_DO_Zone \\\n",
    "                    ORDER BY count(1) DESC\")\n",
    "sqlDF.show(20, False) # to show the full column contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b0e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2794b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8b598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
